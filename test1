from transformers import pipeline
import requests
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
import streamlit as st

# NLTK-Ressourcen herunterladen
nltk.download("punkt")
nltk.download("stopwords")

# NER-Modell initialisieren
ner_pipeline = pipeline("ner", model="dbmdz/bert-base-german-cased", aggregation_strategy="simple")

# Deutsche Stoppw√∂rter laden
german_stopwords = set(stopwords.words("german"))

# Zus√§tzliche h√§ufige W√∂rter, die ignoriert werden sollen
frequent_words = {"hatte", "hast", "hat", "ist", "und", "oder", "nicht", "das", "war", "ein", "eine", "der", "die", "mit"}

# Funktion: H√§ufige W√∂rter filtern
def is_common_word(word):
    return word.lower() in german_stopwords or word.lower() in frequent_words

# Funktion: Wiktionary f√ºr Definitionen
def fetch_wiktionary_definition(word):
    api_url = "https://de.wiktionary.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "prop": "extracts",
        "titles": word,
        "explaintext": True
    }

    response = requests.get(api_url, params=params)
    response.raise_for_status()
    data = response.json()

    pages = data.get("query", {}).get("pages", {})
    for _, page_data in pages.items():
        extract = page_data.get("extract", "")
        if extract:
            return extract_meanings_only(extract)

    return "Keine Erkl√§rung gefunden."

# Extrahiere nur den Bedeutungen-Abschnitt
def extract_meanings_only(text):
    match = re.search(r"Bedeutungen:\n(.*?)\n(?:Synonyme:|Beispiele:|Wortbildungen:|\Z)", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    return "Keine Bedeutungen gefunden."

# Schwierige W√∂rter identifizieren
def find_difficult_words(text):
    words = word_tokenize(text)
    filtered_words = [word for word in words if word.isalpha() and len(word) > 6 and not is_common_word(word)]

    detected_words = set()
    results = ner_pipeline(" ".join(filtered_words))

    for entity in results:
        word = entity.get("word")
        # Entferne Pr√§fixe wie ## und stelle sicher, dass das Wort vollst√§ndig bleibt
        if word and not word.startswith("##"):
            full_word = word.strip(".,!?;:()[]{}")
            if full_word.isalpha() and len(full_word) > 6 and not is_common_word(full_word):
                detected_words.add(full_word)

    definitions = {}
    for word in detected_words:
        definitions[word] = fetch_wiktionary_definition(word)

    return definitions

# Hauptfunktion f√ºr Streamlit
def main():
    st.title("üìñ Digitaler Leseassistent - W√∂rter Erkennen und Erkl√§ren")

    text = st.text_area("Gib hier deinen Text ein:", height=200)

    if st.button("üîç W√∂rter analysieren"):
        if text.strip():
            with st.spinner("Analysiere den Text..."):
                definitions = find_difficult_words(text)

            if definitions:
                st.markdown("### üß† Gefundene schwierige W√∂rter:")
                for word, definition in definitions.items():
                    st.markdown(f"- **{word}**: {definition}")
            else:
                st.success("Keine schwierigen W√∂rter erkannt!")
        else:
            st.error("Bitte gib einen Text ein.")

if __name__ == "__main__":
    main()
